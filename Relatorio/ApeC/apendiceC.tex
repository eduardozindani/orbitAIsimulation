\chapter{Voice Pipeline Implementation}
\label{app:voice_implementation}

This appendix provides detailed technical specifications for the bidirectional voice system described in Section~\ref{subsec:voice_implementation}, implementing voice interaction as a hands-free modality for immersive VR environments outlined in Section~\ref{sec:ar_vr_review}. All API endpoints, audio formats, class methods, and processing parameters are verified against the Unity project voice integration code.

\section{System Architecture}
\label{app:voice_architecture}

The voice pipeline implements bidirectional audio through ElevenLabs cloud APIs, enabling natural spoken interaction with the agent system. The data flow follows this sequence: user speech → speech-to-text transcription → agent processing → text-to-speech synthesis → audio playback.

\subsubsection*{Component Responsibilities}

\begin{itemize}
\item \textbf{PromptConsole}: Manages microphone capture, push-to-talk input detection, and audio playback
\item \textbf{ElevenLabsClient}: Handles HTTP communication with ElevenLabs APIs (Assets/Scripts/AI/Services/ElevenLabsClient.cs, 394 lines)
\item \textbf{Unity AudioSource}: Plays synthesized speech through Quest 3's spatial audio system
\item \textbf{Unity Microphone}: Captures user voice input at 16 kHz sample rate
\end{itemize}

\section{Speech-to-Text Pipeline}
\label{app:voice_stt}

Speech recognition converts user voice input to text through ElevenLabs' Scribe v1 transcription model. Table~\ref{tab:stt_specs} documents the audio capture specifications.

\begin{table}[h]
\centering
\caption{Speech-to-Text Audio Capture Specifications}
\label{tab:stt_specs}
\begin{tabular}{ll}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
Sample rate & 16,000 Hz (optimized for speech) \\
Bit depth & 16-bit PCM \\
Channels & Mono \\
Maximum duration & 30 seconds \\
Audio format (transmitted) & WAV with RIFF header \\
API endpoint & \texttt{/speech-to-text} \\
Model & \texttt{scribe\_v1} \\
\hline
\end{tabular}
\end{table}

\subsection{Push-to-Talk Input Detection}

Voice recording activates through push-to-talk button press. The \texttt{PromptConsole.Update()} method (lines 281--323) implements platform-specific input detection with debouncing to prevent accidental double-triggers.

\subsubsection*{Input Source Detection}

\begin{itemize}
\item \textbf{Desktop testing}: Space key via \texttt{Input.GetKeyDown(KeyCode.Space)}
\item \textbf{VR deployment}: Quest 3 right controller A button via \texttt{OVRInput.Get(OVRInput.Button.One, OVRInput.Controller.RTouch)}
\end{itemize}

\subsubsection*{State Machine} Table~\ref{tab:recording_states} documents the recording state transitions.

\begin{table}[h]
\centering
\caption{Recording State Machine}
\label{tab:recording_states}
\begin{tabular}{lll}
\hline
\textbf{State} & \textbf{Trigger} & \textbf{Next State} \\
\hline
Idle & Button press & Recording \\
Recording & Button release & Processing \\
Processing & Transcription complete & Idle \\
\hline
\end{tabular}
\end{table}

During the Recording state, a red visual indicator displays ``Listening...'' to provide user feedback.

\subsection{Audio Capture and Conversion}

The \texttt{StartRecording()} method initiates Unity's \texttt{Microphone.Start()} with the specifications in Table~\ref{tab:stt_specs}. When the user releases the button, \texttt{StopRecordingAndTranscribe()} processes the captured audio.

\subsubsection*{WAV Conversion Algorithm} The \texttt{ConvertAudioClipToWav()} method (lines 322--368) converts Unity's \texttt{AudioClip} format to WAV for API transmission:

\begin{enumerate}
\item Extract float samples from \texttt{AudioClip.GetData()}
\item Convert float [-1.0, 1.0] to 16-bit signed integer [-32768, 32767]
\item Construct RIFF WAV header (44 bytes):
\begin{itemize}
    \item Chunk ID: ``RIFF''
    \item Format: ``WAVE''
    \item Subchunk 1: ``fmt '' (audio format specification)
    \item Subchunk 2: ``data'' (PCM samples)
\end{itemize}
\item Concatenate header + PCM data
\end{enumerate}

\subsection{API Request Structure}

The WAV bytes transmit to ElevenLabs via \texttt{WWWForm} multipart HTTP POST:

\begin{verbatim}
POST https://api.elevenlabs.io/v1/speech-to-text
Content-Type: multipart/form-data
Headers: xi-api-key: [API_KEY]

Body:
  - file: recording.wav (binary WAV data)
  - model_id: "scribe_v1"
\end{verbatim}

\subsection{Response Parsing}

The API returns JSON containing:

\begin{itemize}
\item \textbf{text}: Transcribed text string
\item \textbf{confidence}: Recognition confidence score [0.0--1.0]
\end{itemize}

The transcribed text feeds directly into the agent's \texttt{ProcessUserInput()} method for intent interpretation and tool selection.

\section{Text-to-Speech Pipeline}
\label{app:voice_tts}

Agent text responses convert to speech through ElevenLabs' text-to-speech API. Table~\ref{tab:tts_specs} documents the synthesis configuration.

\begin{table}[h]
\centering
\caption{Text-to-Speech Synthesis Parameters}
\label{tab:tts_specs}
\begin{tabular}{ll}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
Model & \texttt{eleven\_flash\_v2\_5} \\
Stability & 0.7 (voice consistency) \\
Similarity boost & 0.8 (voice clarity) \\
Speed & 1.0 (normal playback) \\
Audio format (received) & MP3 \\
API endpoint & \texttt{/text-to-speech/\{voiceId\}} \\
Synthesis latency & 1--3 seconds (typical) \\
\hline
\end{tabular}
\end{table}

\subsection{API Request Structure}

The \texttt{TextToSpeechAsync()} method (lines 31--129) sends synthesis requests:

\begin{verbatim}
POST https://api.elevenlabs.io/v1/text-to-speech/{voiceId}
Content-Type: application/json
Headers: xi-api-key: [API_KEY]

Body:
{
  "text": "Agent response text here",
  "model_id": "eleven_flash_v2_5",
  "voice_settings": {
    "stability": 0.7,
    "similarity_boost": 0.8,
    "speed": 1.0
  }
}
\end{verbatim}

\subsection{MP3 Decoding and Playback}

The API returns MP3-encoded audio via HTTP response body. The \texttt{ConvertMp3ToAudioClipAsync()} method (lines 135--154) performs decoding:

\begin{enumerate}
\item Write MP3 bytes to temporary file in \texttt{Application.temporaryCachePath}
\item Load via \texttt{UnityWebRequestMultimedia.GetAudioClip(uri, AudioType.MPEG)}
\item Enable streaming mode for memory efficiency
\item Extract \texttt{AudioClip} from request
\item Delete temporary file in \texttt{finally} block
\end{enumerate}

The resulting \texttt{AudioClip} plays through Unity's \texttt{AudioSource} component attached to the camera, utilizing Quest 3's spatial audio capabilities for immersive voice delivery positioned at the user's head location.

\subsection{Model Selection Rationale}

The \texttt{eleven\_flash\_v2\_5} model balances:

\begin{itemize}
\item \textbf{Synthesis speed}: 1--3 seconds for typical 2--3 sentence responses (critical for real-time interaction)
\item \textbf{Voice fidelity}: Natural prosody and intonation
\item \textbf{API cost}: Flash models optimize for speed over maximum quality
\end{itemize}

\section{Character Voice Management}
\label{app:voice_characters}

Each agent character uses a distinct ElevenLabs voice ID configured in \texttt{MissionConfig.specialistVoice} ScriptableObject references. Table~\ref{tab:voice_ids} documents character voice assignments.

\begin{table}[h]
\centering
\caption{Character Voice ID Assignments}
\label{tab:voice_ids}
\begin{tabular}{llp{6cm}}
\hline
\textbf{Character} & \textbf{Voice ID} & \textbf{Characteristics} \\
\hline
Mission Control & \texttt{NOpBlnGInO9m6vDvFkFC} & Authoritative, encouraging, professional \\
Anastasia (ISS Specialist) & \texttt{ZF6FPAbjXT4488VcRRnw} & Professional engineer - clear, technical, friendly \\
Dr. Harrison (Hubble Specialist) & \texttt{M4zkunnpRihDKTNF0D7f} & Veteran aerospace engineer - technical, experienced, proud of Hubble's legacy \\
Karl (Voyager Specialist) & \texttt{t1oG32lG6Z6edP2XJLiz} & Philosophical scientist and cosmic poet - contemplative, poetic, awe-inspiring \\
\hline
\end{tabular}
\end{table}

\subsection{Scene-Specific Voice Switching}

When users invoke the \texttt{route\_to\_mission} tool, the scene transition loads mission-specific \texttt{ElevenLabsSettings} assets that override the default voice ID. This ensures:

\begin{itemize}
\item Hub agent responses use Mission Control voice
\item ISS Mission Space responses use Anastasia's voice profile
\item Each specialist maintains consistent vocal identity
\end{itemize}

Voice synthesis parameters (stability, similarity boost, speed) remain constant across all characters to maintain audio quality consistency, while the underlying voice models provide tonal and character differentiation.

\subsection{Voice Settings Persistence}

The \texttt{ElevenLabsClient} caches the current \texttt{ElevenLabsSettings} reference. Scene transitions update this reference automatically through Unity's scene loading hooks, enabling seamless character voice switching without code changes in the agent logic.

\section{Error Handling and Fallbacks}
\label{app:voice_errors}

The voice pipeline implements robust error handling for network failures and API timeouts:

\subsubsection*{Speech-to-Text Errors}

\begin{itemize}
\item \textbf{Microphone unavailable}: Display error message, fall back to text input
\item \textbf{API timeout}: Retry once with exponential backoff, then show error
\item \textbf{Low confidence score}: Accept transcription but log warning
\end{itemize}

\subsubsection*{Text-to-Speech Errors}

\begin{itemize}
\item \textbf{API timeout}: Display text response without audio
\item \textbf{MP3 decode failure}: Log error, display text fallback
\item \textbf{AudioSource unavailable}: Silent failure, text remains visible
\end{itemize}

All errors log to Unity console with structured error messages for debugging while maintaining graceful degradation of user experience.

\section{Performance Optimization}
\label{app:voice_performance}

\subsection{Memory Management}

\begin{itemize}
\item Temporary WAV/MP3 files deleted immediately after use
\item \texttt{AudioClip} instances released when playback completes
\item Streaming mode for MP3 decoding reduces peak memory usage
\item No audio caching (prioritizes memory over latency)
\end{itemize}

\subsection{Latency Budget}

Table~\ref{tab:voice_latency} documents typical latency components for the complete voice interaction cycle.

\begin{table}[h]
\centering
\caption{Voice Interaction Latency Budget}
\label{tab:voice_latency}
\begin{tabular}{lr}
\hline
\textbf{Component} & \textbf{Latency} \\
\hline
User speech duration & Variable (user-controlled) \\
WAV conversion & < 100 ms \\
STT API request & 500--1500 ms \\
Agent processing (GPT-4.1) & 1000--3000 ms \\
TTS API request & 1000--3000 ms \\
MP3 decode & < 200 ms \\
Audio playback start & < 50 ms \\
\hline
\textbf{Total (excluding user speech)} & \textbf{2.5--7.8 seconds} \\
\hline
\end{tabular}
\end{table}

The 2.5--7.8 second response time falls within acceptable bounds for educational conversational interfaces, where thoughtful responses outweigh instantaneous feedback.
