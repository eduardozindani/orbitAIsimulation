\section{Core Module Implementation}
\label{sec:core_implementation}

This section describes how the platform's four primary modules—conversational agent, orbital physics simulation, voice integration, and virtual reality environment—work together to create an immersive educational experience for learning orbital mechanics. Each subsection explains the educational rationale behind key design decisions and how they support the learning objectives established in Section~\ref{sec:methodology}. Complete technical specifications are provided in Appendices~\ref{app:agent_implementation}--\ref{app:vr_implementation}.

\subsection{Agent System Implementation}
\label{subsec:agent_implementation}

The conversational agent system removes the traditional barrier between learning intent and technical execution by enabling natural language control of orbital simulations. Learners can express goals like ``Create an orbit matching the ISS'' or ``Show me a highly elliptical orbit'' without needing to understand programming, coordinate systems, or simulation APIs. This design decision directly addresses a core challenge in physics education: allowing students to focus on conceptual understanding rather than technical syntax.

Following the tool-calling architecture outlined in Section~\ref{sec:generative-agents}, the system implements OpenAI GPT-4 with a structured tool-calling framework that interprets user requests and invokes validated simulation commands. When a learner asks to create an orbit, the agent translates natural language into precise physics parameters (altitude, inclination, eccentricity), executes the orbital calculation, and explains the result in educational terms. Critically, the agent disambiguates between orbital velocity (the speed required to maintain a specific orbit, calculated from physics) and simulation time speed (how fast the visualization plays back)—a common source of confusion that this explicit separation prevents.

The platform embodies two agent archetypes: Mission Control (at the Hub) focuses on orbit creation and simulation control, while three mission specialists (ISS, Hubble, Voyager) provide mission-specific educational context when learners navigate to dedicated Mission Spaces. This dual-character design supports two learning modes: hands-on experimentation at the Hub, and contextual deepening through mission-specific dialogue. Conversation history persists across scene transitions, enabling learners to ask follow-up questions like ``What was the altitude of the orbit I just created?'' after switching contexts—supporting iterative, exploratory learning patterns.

Technical implementation details, including prompt architecture, tool schemas, context management algorithms, and API integration specifications, are documented in Appendix~\ref{app:agent_implementation}.

\subsection{Orbital Physics Simulation}
\label{subsec:physics_implementation}

The orbital physics engine translates altitude specifications into velocity requirements automatically, making visible a fundamental relationship that students often struggle to grasp: that orbital speed is not arbitrary but determined by altitude through gravitational physics. When a learner requests ``an orbit at 420 km like the ISS,'' the system calculates the required velocity (7.66 km/s) using the vis-viva equation and displays both values together. This automatic calculation prevents a common misconception—that higher orbits move faster—by immediately showing that geostationary satellites at 35,786 km altitude actually travel slower (3.07 km/s) than low Earth orbit satellites, despite their greater distance.

The simulation implements two-body Keplerian mechanics (Section~\ref{sec:orbital_mechanics}) with physics-consistent trajectory calculations for circular and elliptical orbits. Visual trajectories render as continuous curves in VR space, allowing learners to observe geometric properties directly: circular orbits maintain constant radius, while elliptical orbits visually demonstrate eccentricity through their oblong shape. Scale compression maps Earth's 6,371 km radius to a comfortable VR viewing volume while preserving proportional relationships—the ISS appears at 6.6\% of Earth's radius above the surface, matching the real ratio—enabling learners to develop accurate spatial intuition about orbital altitudes without being overwhelmed by vast scales.

Critically, all physics calculations occur in real units (km, km/s) before conversion to rendering space, ensuring that displayed values match published orbital data for ISS, Hubble, and other missions. This fidelity allows learners to verify simulation results against authoritative sources, building confidence in the educational tool. Complete physics implementation, including vis-viva equation derivations, scale compression algorithms, and trajectory visualization methods, appears in Appendix~\ref{app:physics_implementation}.

\subsection{Voice Integration Pipeline}
\label{subsec:voice_implementation}

Voice interaction addresses a practical constraint of immersive VR environments (Section~\ref{sec:ar_vr_review}): hands holding controllers cannot easily type, and virtual keyboards break immersion. The system implements bidirectional speech through push-to-talk input (Quest 3 controller A button) and synthesized character voices, enabling learners to engage in natural spoken dialogue while manipulating 3D orbital visualizations. This hands-free modality supports exploratory learning patterns where students voice hypotheses (``What happens if I increase the altitude?''), observe results, and refine understanding through iterative questioning—a cognitive process difficult to sustain when switching between physical keyboards and immersive VR.

Each agent character embodies a distinct voice: Mission Control speaks with authoritative encouragement at the Hub, while mission specialists (like Anastasia, the ISS expert) adopt personalities aligned with their educational roles—professional, technical, and approachable. This character differentiation serves pedagogical purposes beyond engagement: learners develop associative memory between voice identity and knowledge domain, reinforcing context switching as they navigate between experimental workspace (Hub) and mission-specific deepening (ISS, Hubble, Voyager spaces). Voice synthesis occurs within 1--3 seconds of agent response generation, maintaining conversational flow without perceptible delays that would disrupt the learning dialogue.

The push-to-talk mechanism balances spontaneity with intentionality: learners explicitly signal when they wish to speak, preventing accidental voice activation while preserving the natural rhythm of conversation. This design choice emerged from recognizing that educational dialogue differs from commercial voice assistants—students need time to think between questions, and the platform should not interpret silence as disengagement. Technical details of speech-to-text processing, audio synthesis parameters, and character voice management appear in Appendix~\ref{app:voice_implementation}.

\subsection{Virtual Reality Environment}
\label{subsec:vr_implementation}

As discussed in Section~\ref{sec:ar_vr_review}, the virtual reality environment transforms abstract orbital mechanics into spatial experiences that leverage human depth perception and proprioception. Orbits exist as three-dimensional curves that learners can walk around, crouch beneath, and observe from multiple vantage points—building geometric intuition impossible to achieve through 2D screens or static diagrams. Seeing an elliptical orbit's eccentricity from different angles, or observing how inclination tilts the orbital plane relative to Earth's equator, engages spatial reasoning faculties that support conceptual understanding of orbital geometry.

The platform deploys to Meta Quest 3, a standalone VR headset enabling tetherless movement around orbital visualizations without PC connection constraints. Maintaining 90 Hz stereoscopic rendering ensures visual comfort during extended learning sessions, preventing the nausea and fatigue that would undermine educational effectiveness. This frame rate requirement drove architectural decisions throughout the implementation: single-pass instanced rendering reduces GPU overhead, texture compression minimizes memory bandwidth, and asynchronous scene loading prevents visible stuttering during navigation between Hub and Mission Spaces.

The multi-scene architecture supports distinct learning contexts: the Hub provides an experimental workspace for orbit creation and manipulation, while three Mission Spaces (ISS, Hubble, Voyager) offer focused environments for deepening understanding of specific missions. Scene transitions preserve conversation history and simulation state, allowing learners to seamlessly shift between hands-on experimentation and contextual exploration. Spatial UI elements render in 3D world space rather than head-locked overlays, maintaining presence and spatial grounding while providing necessary information—mission elapsed time, simulation speed, and dialogue responses appear as objects in the environment rather than disconnected interface chrome.

Technical specifications for Quest 3 deployment, including Android build configuration, input system implementation, stereo rendering pipeline, and performance optimization strategies, are detailed in Appendix~\ref{app:vr_implementation}.
