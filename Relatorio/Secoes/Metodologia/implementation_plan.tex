\section{Implementation Plan}
\label{sec:implementation_plan}

This section outlines the implementation plan for the agent-guided orbital mechanics simulation platform. The development is structured in four focused phases, progressing from core conversational AI to immersive VR deployment. This approach prioritizes fundamental functionality over feature breadth, ensuring each phase delivers a testable, working system that builds toward the complete educational experience.

The plan reflects the principles established in Section \ref{sec:design_philosophy}: prototype-driven development, iterative refinement, and modular architecture. Each phase produces an independently functional component that integrates with the broader system.

\subsection{System Architecture Overview}
\label{subsec:system_overview}

The platform consists of two primary user experiences:

\textbf{The Hub (Mission Control):} A central workspace where users interact with Mission Control to create custom orbits, control simulation parameters, and receive guidance. The Hub presents Earth in space with the ability to visualize a single orbital trajectory at a time. Mission Control, embodied with a visionary persona, coordinates all activities and routes users to specialized learning environments when appropriate.

\textbf{Mission Showcase Spaces:} Separate educational environments dedicated to historical space missions. Each space features a mission specialist who greets users, displays the mission's orbit, and answers questions about mission-specific physics and engineering. These spaces serve as interactive tutorials where users learn orbital mechanics principles through real-world examples.

Users navigate between the Hub and Mission Spaces through natural dialogue. The system maintains context across transitions, ensuring specialists understand why the user was routed to them and can provide relevant educational content.

\subsection{Phase 1: Core Agent System and Hub Experience}
\label{subsec:phase1_agent}

\subsubsection*{Objectives}

Phase 1 establishes the conversational AI foundation and primary user workspace. The goal is to create Mission Control as an intelligent coordinator capable of guiding orbit creation, managing simulation state, and routing users to educational resources. This phase delivers the fundamental interaction model that all subsequent phases build upon.

\subsubsection*{Key Components}

\paragraph{Mission Control Character}

Mission Control serves as the primary conversational interface with a visionary, encouraging personality. Its core responsibilities include:

\begin{itemize}
    \item Guiding users through orbit creation via structured dialogue
    \item Validating orbital parameters against physical constraints
    \item Executing simulation control commands (time manipulation, orbit clearing)
    \item Routing users to mission specialists when educational context would be valuable
    \item Answering general questions about orbital mechanics
\end{itemize}

The character's prompt engineering emphasizes educational scaffolding—helping users who are uncertain while accepting precise commands from knowledgeable users without unnecessary questioning.

\paragraph{Orbit Creation Workflow}

The system guides users through creating circular or elliptical orbits via natural conversation. The workflow adapts to user knowledge level:

\begin{description}
    \item[Precise Users:] Accept complete specifications directly. Example: "Create a 400km circular orbit at 51.6 degrees" immediately generates the orbit.
    \item[Guided Users:] Ask clarifying questions to gather parameters. Example: User says "I want to observe Earth" → Mission Control suggests polar orbit → guides through altitude selection → creates orbit.
\end{description}

The dialog system must determine orbit type (circular vs. elliptical) and collect required parameters:
\begin{itemize}
    \item Circular orbits: altitude, inclination
    \item Elliptical orbits: periapsis altitude, apoapsis altitude, inclination
\end{itemize}

Parameter validation ensures physical realism (minimum altitude above atmosphere, maximum within simulation bounds). The system displays only one orbit at a time, emphasizing focused exploration over complex multi-satellite scenarios.

\paragraph{Tool Registry and Execution}

Building on the existing tool system architecture, Phase 1 implements:

\begin{itemize}
    \item \texttt{create\_circular\_orbit(altitude\_km, inclination\_deg)} — Generates circular orbital trajectory
    \item \texttt{create\_elliptical\_orbit(periapsis\_km, apoapsis\_km, inclination\_deg)} — Generates elliptical trajectory
    \item \texttt{clear\_orbit()} — Removes current orbit visualization, returns to empty workspace
\end{itemize}

Each tool validates parameters, calculates orbital velocity using the vis-viva equation, and renders the trajectory around Earth. Tool schemas define parameter constraints, enabling the agent to guide users toward valid configurations.

\paragraph{Context Management}

The system tracks minimal state information:
\begin{itemize}
    \item Current location (Hub or specific Mission Space)
    \item Active character (Mission Control or mission specialist)
    \item Current orbit parameters (if any orbit is displayed)
    \item Recent conversation history (last 2-3 exchanges for contextual continuity)
\end{itemize}

This lightweight approach maintains coherent dialogue without introducing complexity that could lead to inconsistent behavior.

\subsubsection*{Success Criteria}

Phase 1 is complete when:
\begin{itemize}
    \item Users can create circular orbits through natural dialogue
    \item Users can create elliptical orbits through natural dialogue
    \item Mission Control adapts to both precise and vague user input appropriately
    \item Parameter validation prevents physically impossible configurations
    \item The clear orbit function resets the workspace correctly
    \item Conversation remains coherent across multiple turns
\end{itemize}

\subsubsection*{Implementation Notes}

Development occurs on desktop Unity first, using text-based interaction for rapid iteration. Voice integration comes in Phase 3. Focus remains on dialogue logic, tool execution correctness, and educational quality of Mission Control's responses.

\subsection{Phase 2: Mission Showcase Spaces}
\label{subsec:phase2_missions}

\subsubsection*{Objectives}

Phase 2 creates educational environments showcasing historical space missions. Each Mission Space features a specialist character who introduces their mission, displays its orbit, and answers questions. These spaces transform abstract physics into concrete examples, allowing users to learn by exploring real-world applications of orbital mechanics.

\subsubsection*{Mission Definitions}

The system implements four to five mission showcases:

\paragraph{ISS Mission Space}
\begin{itemize}
    \item Orbit: Circular, 420 km altitude, 51.6° inclination
    \item Specialist: Professional astronaut/engineer persona
    \item Educational focus: Low Earth Orbit characteristics, launch site constraints, international collaboration, orbital period calculation
\end{itemize}

\paragraph{GPS Mission Space}
\begin{itemize}
    \item Orbit: Circular, 20,200 km altitude, 55° inclination
    \item Specialist: Navigation engineer persona
    \item Educational focus: Medium Earth Orbit, constellation design principles, global coverage geometry, atomic clock precision requirements
\end{itemize}

\paragraph{Voyager Mission Space}
\begin{itemize}
    \item Orbit: Escape trajectory (hyperbolic) or high elliptical approximation
    \item Specialist: Wise scientist persona (Carl Sagan style)
    \item Educational focus: Interplanetary trajectories, gravity assists, escape velocity, deep space navigation
\end{itemize}

\paragraph{Hubble Mission Space}
\begin{itemize}
    \item Orbit: Circular, 540 km altitude, 28.5° inclination
    \item Specialist: Astrophysicist persona
    \item Educational focus: Low inclination benefits, orbital maintenance, atmospheric drag at altitude, telescope operations
\end{itemize}

\paragraph{Apollo 11 Mission Space (Optional)}
\begin{itemize}
    \item Orbit: Trans-lunar trajectory (if technically feasible within scope)
    \item Specialist: Astronaut commander persona (Neil Armstrong style)
    \item Educational focus: Lunar transfer orbits, escape from Earth, orbital insertion at Moon
    \item Note: Implementation complexity may require deferring to future work
\end{itemize}

\subsubsection*{Specialist Character Behavior}

When a user arrives at a Mission Space:

\begin{enumerate}
    \item The mission's orbit automatically displays
    \item The specialist greets the user with a brief introduction (15-20 seconds)
    \item The greeting includes mission history highlights and current visualization context
    \item The specialist invites questions: "I'm here to explain anything about this mission"
    \item The specialist answers questions using mission-specific knowledge
    \item The specialist can route back to Hub when user says "go back" or similar
\end{enumerate}

Each specialist is prompt-engineered with deep knowledge of their specific mission, ensuring accurate answers to questions about orbital parameters, mission objectives, historical significance, and underlying physics.

\subsubsection*{Routing System}

Mission Control intelligently routes users to specialists:

\begin{description}
    \item[User requests specific mission:] "Show me the ISS" → Direct routing to ISS Space
    \item[User asks question better answered by specialist:] "Why do polar orbits cover the whole Earth?" → Mission Control: "The GPS specialist can explain constellation coverage perfectly. Want me to connect you?"
    \item[User explores blindly:] Mission Control can suggest: "Would you like to see how real missions use these principles? I can introduce you to specialists from ISS, GPS, Voyager, or Hubble."
\end{description}

The routing system passes context: when a specialist greets a user, they know why the user was sent and can reference that context ("Mission Control mentioned you were curious about polar orbits—let me show you how GPS uses that...").

\subsubsection*{Mission Data Structure}

Each mission is defined via configuration:

\begin{verbatim}
{
  "mission_id": "iss",
  "orbit_parameters": {
    "type": "circular",
    "altitude_km": 420,
    "inclination_deg": 51.6
  },
  "specialist_persona": "professional_astronaut",
  "brief_history": "Launched 1998, continuous operation...",
  "key_educational_points": [
    "Orbital period: 90 minutes",
    "Inclination allows launches from multiple sites",
    "Velocity: 7.6 km/s"
  ]
}
\end{verbatim}

This declarative structure allows adding new missions without code changes.

\subsubsection*{Success Criteria}

Phase 2 is complete when:
\begin{itemize}
    \item All implemented mission spaces display correct orbits on arrival
    \item Specialists deliver appropriate greeting messages
    \item Users can ask questions and receive educationally sound answers
    \item Routing from Hub to missions works seamlessly
    \item Routing from missions back to Hub works seamlessly
    \item Context is preserved across space transitions
\end{itemize}

\subsection{Phase 3: Voice Integration}
\label{subsec:phase3_voice}

\subsubsection*{Objectives}

Phase 3 transforms text-based interaction into natural spoken dialogue. All character responses are synthesized with distinct voices that match their personas, and users provide input via speech recognition. This phase also implements the opening cutscene that welcomes users to the simulation.

\subsubsection*{Text-to-Speech Implementation}

The system integrates ElevenLabs API for voice synthesis. Each character is assigned a voice profile:

\begin{description}
    \item[Mission Control:] Visionary leader voice (Elon Musk style) — authoritative yet encouraging
    \item[ISS Specialist:] Professional engineer voice — clear, technical, friendly
    \item[GPS Specialist:] Technical expert voice — precise, methodical
    \item[Voyager Specialist:] Wise scientist voice (Carl Sagan style) — poetic, contemplative
    \item[Hubble Specialist:] Scientific enthusiast voice — passionate about astronomy
    \item[Apollo 11 Specialist:] Heroic astronaut voice (Neil Armstrong style) — calm, confident
\end{description}

Voice selection prioritizes naturalness and distinctiveness. Users should immediately recognize which character is speaking based on voice alone.

The TTS pipeline operates asynchronously: as the agent generates text, it's sent to ElevenLabs for synthesis, and audio playback begins as soon as the first chunk arrives, minimizing perceived latency.

\subsubsection*{Speech-to-Text Implementation}

User input is captured via speech recognition. Implementation options include:
\begin{itemize}
    \item OpenAI Whisper API (cloud-based, high accuracy)
    \item Meta Quest 3 native speech recognition (low-latency, on-device)
    \item Unity platform-specific solutions
\end{itemize}

Selection prioritizes accuracy and latency. A push-to-talk activation model (via controller button) prevents accidental triggering and provides clear interaction boundaries.

Visual transcription displays what the system understood, allowing users to verify correct recognition before the agent processes the command.

\subsubsection*{Opening Cutscene}

The first user experience is a 40-second pre-scripted introduction:

\begin{itemize}
    \item Delivered by Mission Control (Elon Musk voice)
    \item Welcomes user to the orbital mechanics simulator
    \item Explains the learning philosophy: exploration over instruction
    \item Previews capabilities: create orbits, explore missions, ask questions
    \item Motivational close emphasizing the beauty of physics and spaceflight
\end{itemize}

The cutscene script is pre-written, synthesized once via ElevenLabs, and stored as an audio asset. This ensures consistent quality and eliminates API latency for the critical first impression.

After the cutscene completes, the user seamlessly transitions to the Hub where Mission Control awaits.

\subsubsection*{Success Criteria}

Phase 3 is complete when:
\begin{itemize}
    \item All character dialogue is spoken with appropriate voices
    \item Voices are clearly distinguishable between characters
    \item Speech recognition achieves >90\% accuracy on orbital mechanics vocabulary
    \item Audio playback is smooth without noticeable lag
    \item Opening cutscene plays on first launch and sounds professional
    \item Users can complete full interactions using only voice (no text fallback required)
\end{itemize}

\subsection{Phase 4: VR Deployment and Essential Controls}
\label{subsec:phase4_vr}

\subsubsection*{Objectives}

Phase 4 deploys the complete system to Meta Quest 3, creating an immersive VR experience. Users don the headset, speak naturally to Mission Control, visualize orbits in three-dimensional space, and physically move through environments. This phase also implements essential simulation controls and performance optimization.

\subsubsection*{VR Platform Configuration}

The Unity project is configured for Meta Quest 3 deployment:
\begin{itemize}
    \item Android build target with ARM64 architecture
    \item Meta XR SDK integration via OpenXR
    \item Universal Render Pipeline (URP) optimized for mobile VR
    \item Texture compression (ASTC) and resolution management
    \item Build pipeline via Meta Quest Developer Hub
\end{itemize}

The VR camera rig supports six degrees of freedom (6-DOF) head tracking, stereo rendering for depth perception, and guardian boundary integration for safety.

\subsubsection*{Spatial User Interface}

Interface elements are redesigned for three-dimensional space:
\begin{itemize}
    \item Floating orbital parameter displays near satellite positions
    \item Controller-attached radial menus for quick command access
    \item World-locked panels for mission briefings that remain fixed in space
    \item Visual transcription of speech input displayed in comfortable viewing position
\end{itemize}

Text is rendered at sizes readable from 1-3 meters away. High contrast and background panels ensure legibility against varying backgrounds (Earth, space, orbital trails).

\subsubsection*{Controller Integration}

Quest 3 Touch Pro controllers enable direct interaction:
\begin{itemize}
    \item Grip button: Push-to-talk activation for voice input
    \item Trigger: Ray-cast selection of UI elements
    \item Thumbstick: Navigate through space (approach or retreat from Earth)
    \item Face buttons: Quick access to time controls and menu
\end{itemize}

Visual rays indicate controller pointing direction. Haptic feedback confirms selections and state changes.

\subsubsection*{Simulation Control Tools}

Essential control functions are implemented:

\paragraph{Time Manipulation}
\begin{itemize}
    \item \texttt{pause\_simulation()} — Freeze orbital motion
    \item \texttt{set\_time\_speed(multiplier)} — Accelerate time (2x, 5x, 10x, etc.)
    \item \texttt{resume\_simulation()} — Return to normal speed
\end{itemize}

Time controls allow users to observe multiple orbital periods quickly or pause to examine specific configurations. Rewind functionality is considered optional; if implementation proves complex, it can be deferred.

\paragraph{Workspace Management}
\begin{itemize}
    \item \texttt{clear\_orbit()} — Remove current orbit, return to empty Hub
\end{itemize}

This command provides a clean slate for new explorations without requiring complex multi-orbit management.

All controls are accessible via voice commands or controller buttons, ensuring multiple interaction pathways.

\subsubsection*{Performance Optimization}

The application targets 72 Hz minimum frame rate (Quest 3 baseline) with 90 Hz as the optimization goal. Profiling and optimization focus on:

\begin{itemize}
    \item Level-of-detail (LOD) systems for orbital trails and satellite models
    \item Asynchronous agent API calls with loading indicators to prevent frame drops
    \item Texture atlasing and mesh batching to reduce draw calls
    \item Shader complexity reduction (avoiding expensive fragment operations)
    \item Memory management: object pooling, asset unloading
\end{itemize}

Unity Profiler and Quest 3 performance overlay identify bottlenecks for targeted optimization.

\subsubsection*{Immersive Environment}

The VR environment creates presence:
\begin{itemize}
    \item High-resolution starfield skybox (8K Milky Way texture)
    \item Photorealistic Earth model with day/night textures
    \item Orbital trails rendered as glowing curves in 3D space
    \item Ambient space audio (subtle background, thruster sounds during maneuvers)
    \item Dynamic lighting from the Sun
\end{itemize}

Earth and orbital distances use compressed scale for VR comfort—maintaining geometric accuracy while fitting within comfortable viewing volume (preventing extreme near/far clipping issues).

\subsubsection*{Augmented Reality Mode (Optional)}

If development timeline permits, AR passthrough mode is implemented:
\begin{itemize}
    \item Quest 3 color passthrough enables viewing real environment
    \item Orbital visualizations can be anchored to a physical location (e.g., table-mounted globe)
    \item Spatial anchors provide persistence across sessions
    \item User can toggle between VR and AR modes seamlessly
\end{itemize}

This feature is considered a valuable enhancement but not essential for core functionality. VR-only deployment satisfies all thesis objectives.

\subsubsection*{Success Criteria}

Phase 4 is complete when:
\begin{itemize}
    \item Application builds and runs on Meta Quest 3 without crashes
    \item Frame rate maintains ≥72 Hz during normal usage
    \item All agent features (orbit creation, mission spaces, voice) function in VR
    \item Users can complete full workflows entirely in VR
    \item Controller and voice inputs work reliably
    \item Time controls (pause, speed, clear) execute correctly
    \item No motion sickness reported by test users during 15-minute sessions
\end{itemize}

\subsection{Development Timeline}
\label{subsec:timeline}

Table \ref{tab:implementation_timeline} summarizes the estimated timeline for the four-phase development plan.

\begin{table}[h]
\centering
\caption{Four-phase implementation timeline.}
\label{tab:implementation_timeline}
\begin{tabular}{@{}llc@{}}
\toprule
\textbf{Phase} & \textbf{Focus Area} & \textbf{Duration (weeks)} \\
\midrule
Phase 1 & Core Agent \& Hub & 3--4 \\
Phase 2 & Mission Showcase Spaces & 2--3 \\
Phase 3 & Voice Integration & 2--3 \\
Phase 4 & VR Deployment \& Controls & 3--4 \\
\midrule
\textbf{Total} & & \textbf{10--14} \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Development Strategy}

Phases 1 and 2 can proceed sequentially on desktop Unity, establishing the conversational AI foundation before adding sensory modalities. Phase 3 (voice) builds on the completed dialogue system. Phase 4 (VR) integrates all previous work into the immersive platform.

Some parallelization is possible: Mission Space content (Phase 2) can be developed concurrently with dialogue refinement (Phase 1), as they share the same technical foundation but operate in different contexts.

The timeline assumes full-time development focus. Phases can be extended if integration challenges arise or compressed if components prove simpler than anticipated.

\paragraph{Risk Mitigation}

\begin{description}
    \item[Agent Response Quality:] Extensive prompt engineering and testing with diverse user inputs ensures robust dialogue. Fallback responses handle edge cases gracefully.
    \item[Voice Recognition Accuracy:] Visual transcription allows users to verify recognition. Text input remains available as backup.
    \item[Performance on Quest 3:] Early profiling and aggressive optimization target 72 Hz minimum. Desktop fallback available if mobile optimization proves prohibitive.
    \item[Apollo 11 Complexity:] Lunar trajectories may exceed scope. Feature is marked optional and can be deferred to future work without impacting core platform.
\end{description}

\subsection{Evaluation Approach}
\label{subsec:evaluation_approach}

The platform's success is measured through technical validation and educational assessment, as detailed in Section \ref{sec:evaluation_plan}. The implementation plan ensures all necessary evaluation capabilities:

\paragraph{Technical Metrics}
\begin{itemize}
    \item Agent accuracy: Tool selection correctness, parameter extraction precision
    \item System latency: Voice-to-response pipeline timing
    \item VR performance: Frame rate stability, motion sickness incidence
    \item Routing reliability: Context preservation across space transitions
\end{itemize}

\paragraph{Educational Effectiveness}
\begin{itemize}
    \item User comprehension: Pre/post questionnaires on orbital mechanics concepts
    \item Engagement quality: Qualitative feedback on learning experience
    \item Comparison: Effectiveness versus traditional textbook/diagram methods
\end{itemize}

Data logging throughout the system captures user commands, agent responses, tool executions, and interaction patterns for post-session analysis.

\subsection{Alignment with Project Objectives}
\label{subsec:plan_alignment}

This implementation plan directly realizes the objectives established in Section \ref{sec:objetivos}:

\begin{enumerate}
    \item \textbf{Physically accurate simulation:} Orbital calculations use vis-viva equation and validated astrodynamics. Mission showcases demonstrate real-world applications.
    \item \textbf{Generative agent integration:} Multi-character AI system interprets natural language, executes simulation functions, and provides educational guidance.
    \item \textbf{Augmented reality visualization:} Phase 4 deploys to Quest 3 with optional AR passthrough anchoring.
    \item \textbf{Multimodal interaction:} Voice dialogue (Phase 3) combined with spatial movement and controller input (Phase 4) creates seamless experience.
    \item \textbf{Coherent real-time system:} All components integrate into unified platform with continuous conversational loop.
    \item \textbf{Modular architecture:} Each phase produces standalone functionality that extends previous work without requiring architectural changes.
    \item \textbf{Educational evaluation:} Built-in logging and assessment tools enable formal evaluation per Section \ref{sec:evaluation_plan}.
\end{enumerate}

The four-phase structure prioritizes delivering a complete, working educational platform over feature breadth. The focus remains on creating magical moments—users speaking naturally to Mission Control, exploring real missions with legendary scientists' voices, and understanding orbital mechanics through direct experience rather than abstraction.
