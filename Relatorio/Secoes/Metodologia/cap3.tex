\section{Design Philosophy and Approach}
\label{sec:design_philosophy}

The development of this project is fundamentally an exploratory research endeavour into a new paradigm of human-computer interaction for educational purposes. Given the innovative and complex nature of integrating generative AI, immersive mixed reality, and embodied interfaces, a rigid, waterfall-style development plan would be inappropriate. Instead, the methodology is guided by a philosophy that embraces iteration and modularity to navigate the technical challenges and discovery process inherent in such work.

The approach is defined by three core principles:
\begin{itemize}
    \item \textbf{Prototype-Driven:} The primary goal is the creation of a functional prototype that demonstrates the feasibility and potential of the proposed system. This approach prioritizes implementing the core functionalities of the user experience over exhaustive feature development, allowing for tangible and testable results that can validate the project's central thesis.
    \item \textbf{Iterative Development:} The project will be built in iterative cycles, following a process of building a core feature, testing its performance and usability, and refining it based on the results. This allows for flexibility in the implementation details, acknowledging that the optimal solutions for agent prompting and user interaction will be discovered and improved upon throughout the development lifecycle.
    \item \textbf{Modular Architecture:} The system is designed as a collection of distinct yet interconnected modules: the generative agent (the "brain") and the simulation and visualisation engine (the "world"). This modularity, a key objective of this project, makes the complex system manageable, facilitates parallel development and testing of components, and ensures the final architecture is extensible for future work.
\end{itemize}

These principles guided a four-phase development strategy. Phase 1 established the conversational agent in the Hub environment with circular and elliptical orbit creation capabilities. Phase 2 implemented three Mission Spaces (ISS, Hubble, Voyager) with specialist agents and scene transition tools. Phase 3 integrated bidirectional voice through ElevenLabs (Scribe v2 for speech-to-text, TTS for character synthesis). Phase 4 deployed the complete system to Meta Quest 3 with immersive VR visualization. Each phase produced a testable, working system that integrated seamlessly with subsequent development without requiring architectural changes.

\section{System Architecture and Data Flow}
\label{sec:system_architecture}

The platform architecture comprises two integrated spaces: the "Hub" (Mission Control) where users create custom orbits through conversation, and "Mission Spaces" (ISS, Hubble, Voyager) where specialists demonstrate real missions. Users navigate between these environments via voice commands (\texttt{route\_to\_mission}, \texttt{return\_to\_hub}), with conversational context preserved across transitions.

The system separates conversational intelligence (OpenAI GPT-4.1) from spatial visualization (Unity 3D), connected through a tool-calling interface. The agent interprets natural language, selects appropriate tools (\texttt{create\_circular\_orbit}, \texttt{create\_elliptical\_orbit}, \texttt{set\_simulation\_speed}, \texttt{pause\_simulation}, \texttt{reset\_simulation\_time}, \texttt{clear\_orbit}, \texttt{route\_to\_mission}, \texttt{return\_to\_hub}), and Unity executes the corresponding physics calculations and rendering.

\subsection{Interaction Flow}
\label{subsec:interaction_flow}

Each user interaction follows a nine-step cycle from speech input to audio output:

\begin{enumerate}
    \item User speaks request via Quest 3 microphone
    \item ElevenLabs Scribe v2 transcribes audio to text
    \item OpenAI GPT-4.1 interprets intent and selects appropriate tool
    \item Unity's \texttt{ToolExecutor} validates parameters and invokes corresponding C\# method
    \item \texttt{SceneTransitionManager} loads Mission Space (for \texttt{route\_to\_mission}); \texttt{OrbitController} calculates trajectory using vis-viva equation and renders visualization (for orbit creation)
    \item Tool execution result returns to LLM as feedback
    \item Agent generates educational response based on tool result and conversation context
    \item ElevenLabs TTS synthesizes character-specific voice (Mission Control's authority, ISS specialist's precision, Voyager specialist's Sagan-like wonder)
    \item Audio plays through Quest 3 spatial audio system
\end{enumerate}

Conversation history is maintained across turns and scene transitions, enabling contextual dialogue. Users control all aspects—orbit creation, mission navigation, simulation parameters—through voice alone.

\section{Mixed Reality Design Rationale}
\label{sec:mr_rationale}

While the Quest 3 supports both VR and AR passthrough, the platform emphasizes immersive VR for pedagogical reasons. Orbital mechanics involves scales (420 km for ISS, 35,786 km for geostationary orbit) incompatible with domestic spaces—AR overlays would show trajectories passing through walls and furniture, creating perceptual friction between room-scale and cosmic-scale contexts. VR isolation places users within the orbital environment itself, establishing coherent spatial context where Earth floats in space and trajectories exist in their natural domain. The architecture supports AR passthrough for collaborative learning, museum installations, or classroom demonstrations where physical anchoring adds value, but the primary experience prioritizes the modality best suited to the content.

\section{Technical Implementation}
\label{sec:core_components}

The system implements conversational AI (OpenAI GPT-4.1 for reasoning, ElevenLabs for voice synthesis/transcription) interfaced with Unity 3D physics simulation. The agent embodies four characters—Mission Control in the Hub, plus three mission specialists (ISS crew perspective for LEO operations, Hubble engineer for telescope mission design, Voyager/Sagan persona for interplanetary trajectories)—each with distinct voice profiles and expertise areas. Unity implements two-body orbital physics in C\#, calculates trajectories via vis-viva equation, and renders visualizations on Quest 3. The platform supports circular orbit creation (160-35,786 km altitude) and elliptical orbit creation (periapsis/apoapsis 160-100,000 km), with inclination constrained to 0-180°.

\section{Development and Version Control}
\label{sec:version_control}

The project follows systematic development practices using GitHub for version control. All source code—including Unity C\# scripts, prompt templates, and configuration files—is tracked in a central repository, providing complete history of changes and enabling experimental work through branching without compromising the main project stability. This systematic approach aligns with the iterative development philosophy, where each development cycle's progress is documented and preserved.

\section{Validation and Dissemination Strategy}
\label{sec:evaluation_plan}

The platform's educational effectiveness and technical implementation are validated through demonstration of complete interaction scenarios across all mission spaces. The validation approach prioritizes real-world usage patterns: users exploring orbital mechanics through natural conversation, navigating between Hub and Mission Spaces, and experiencing the full voice-driven VR workflow. The complete implementation is released open-source on GitHub, enabling community validation, adaptation, and extension. Users provide their own API keys for OpenAI and ElevenLabs services, ensuring accessibility without imposed service costs. Comprehensive documentation covers Unity configuration, Quest 3 deployment, and integration procedures.