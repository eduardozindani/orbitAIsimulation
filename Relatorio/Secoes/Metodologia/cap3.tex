\section{Design Philosophy and Approach}
\label{sec:design_philosophy}

The development of this project is fundamentally an exploratory research endeavour into a new paradigm of human-computer interaction for educational purposes. Given the innovative and complex nature of integrating generative AI, augmented reality, and embodied interfaces, a rigid, waterfall-style development plan would be inappropriate. Instead, the methodology is guided by a philosophy that embraces iteration and modularity to navigate the technical challenges and discovery process inherent in such work.

The approach is defined by three core principles:
\begin{itemize}
    \item \textbf{Prototype-Driven:} The primary goal is the creation of a functional prototype that demonstrates the feasibility and potential of the proposed system. This approach prioritizes implementing the core functionalities of the user experience over exhaustive feature development, allowing for tangible and testable results that can validate the project's central thesis.
    \item \textbf{Iterative Development:} The project will be built in iterative cycles, following a process of building a core feature, testing its performance and usability, and refining it based on the results. This allows for flexibility in the implementation details, acknowledging that the optimal solutions for agent prompting and user interaction will be discovered and improved upon throughout the development lifecycle.
    \item \textbf{Modular Architecture:} The system is designed as a collection of distinct yet interconnected modules: the generative agent (the "brain") and the simulation and visualisation engine (the "world"). This modularity, a key objective of this project, makes the complex system manageable, facilitates parallel development and testing of components, and ensures the final architecture is extensible for future work.
\end{itemize}

\section{System Architecture and Data Flow}
\label{sec:system_architecture}

This section describes the system's architecture and how information flows between the user, the conversational agent, and the visualization engine to create an integrated educational experience.

\paragraph{Architectural Philosophy}

The system is designed around voice-first interaction because spoken dialogue is the most natural way humans seek and share knowledge. Rather than requiring users to learn complex interfaces or command syntax, the platform allows them to simply ask questions and request visualizations as they would when speaking with a knowledgeable instructor.

The architecture separates conversational intelligence from spatial visualization, enabling each component to excel at its specific purpose. The agent interprets intent, reasons about orbital mechanics, and guides learning through dialogue. The simulation engine translates these conversations into visual demonstrations, rendering orbital trajectories in three-dimensional space where users can observe and explore them.

\paragraph{Interaction Flow}

User interactions follow a continuous conversational loop. The user speaks a request—perhaps asking to see a specific orbit or questioning why certain parameters produce particular behaviors. The agent, powered by OpenAI's language models, interprets this natural language input and determines the appropriate response, whether that means creating a visualization, explaining a concept, or routing the user to a relevant learning resource.

When visualization is needed, the agent communicates with the Unity-based simulation engine to generate the requested orbital trajectory. The simulation calculates physically accurate paths using astrodynamics principles and renders them in the virtual reality environment. The user sees the orbit appear in space, anchored to their surroundings through the Meta Quest 3 headset.

The agent then provides spoken feedback via ElevenLabs voice synthesis, confirming what was created and offering educational context. This completes one cycle of the interaction loop, with the user free to ask follow-up questions, request modifications, or explore new concepts.

This architecture enables seamless integration between abstract physics concepts and tangible spatial representations, allowing users to learn through the natural combination of conversation and visual exploration.

\section{Core Component Implementation}
\label{sec:core_components}

The system architecture is realized through two distinct yet deeply integrated components. This separation enables each component to focus on its core responsibility while working together to create the complete educational experience.

\subsection{The Generative Agent (The "Brain")}
\label{subsec:agent}

\paragraph{Purpose and Responsibility}

The conversational agent serves as the educational intelligence of the platform. Its fundamental purpose is to bridge the gap between natural human communication and precise simulation commands, transforming vague or exploratory questions into specific orbital visualizations and educational explanations.

This component must interpret ambiguous user intent—understanding whether "show me how satellites work" should produce a specific orbit example, route to a mission specialist, or provide a conceptual explanation. It maintains conversational context across multiple turns, remembers what has been discussed, and adapts its guidance to match the user's demonstrated knowledge level.

The agent embodies different characters depending on context: Mission Control in the Hub, or mission specialists in the educational spaces. Each character has distinct personality and expertise, created through careful prompt design that defines their communication style, knowledge domain, and educational approach.

\paragraph{Technical Foundation}

The agent's reasoning capabilities are powered by OpenAI's language models, which provide natural language understanding and generation. Voice synthesis through ElevenLabs transforms text responses into spoken dialogue with character-specific voices, creating the auditory personality that makes each character feel distinct and engaging.

The conversational intelligence operates by interpreting user requests, determining appropriate actions (creating orbits, explaining concepts, routing to specialists), and generating educational responses that contextualize what the user sees. This reasoning process must balance precision—ensuring orbital parameters are physically valid—with flexibility to handle the natural variability of spoken language.

\subsection{The Simulation and AR Visualisation (The "World")}
\label{subsec:simulation}

\paragraph{Purpose and Responsibility}

The simulation component translates abstract orbital mechanics into tangible visual experiences. Its fundamental purpose is to make invisible physics visible—showing users what a 400 km orbit actually looks like in space, how inclination affects ground coverage, and how different mission requirements manifest as specific trajectory shapes.

This component must render physically accurate orbital paths while presenting them in ways that are visually comprehensible and educationally meaningful. The orbits must be mathematically correct according to astrodynamics principles, yet scaled and styled for effective learning rather than literal representation.

The visualization creates the spatial context where learning happens. By anchoring orbital trajectories to the user's physical environment through augmented reality, or placing users inside immersive virtual space, the component transforms orbital mechanics from equations on paper into three-dimensional phenomena users can walk around, observe from multiple angles, and genuinely inhabit.

\paragraph{Technical Foundation}

The simulation is built using Unity, a 3D development engine that provides the tools for calculating orbital trajectories and rendering them in virtual and augmented reality. The physics implementation, written in C\#, applies fundamental astrodynamics equations—primarily the two-body problem—to compute satellite positions and velocities over time.

Deployment targets the Meta Quest 3 headset, which enables both immersive VR experiences and AR visualizations through its passthrough camera system. This hardware choice provides standalone wireless operation and sufficient processing power for real-time orbital calculations and high-quality rendering, allowing users to engage with the simulation untethered and freely mobile.

\section{Development and Version Control}
\label{sec:version_control}

The project follows systematic development practices using GitHub for version control. All source code—including Unity C\# scripts, prompt templates, and configuration files—is tracked in a central repository, providing complete history of changes and enabling experimental work through branching without compromising the main project stability. This systematic approach aligns with the iterative development philosophy, where each development cycle's progress is documented and preserved.

\section{Demonstration and Release Strategy}
\label{sec:evaluation_plan}

\paragraph{Community-Driven Validation Rationale}

The platform's ultimate value lies in its real-world application by educators, students, and developers across diverse contexts. Rather than limiting validation to a controlled study, this work adopts a demonstration and open-source release strategy designed to enable broader community engagement, adaptation, and extension. This approach allows the system to be tested and refined through authentic use cases while inviting collaboration from those who can benefit from and contribute to its development.

\subsection{Demonstration Video}

To showcase the platform's capabilities and interaction paradigm, a comprehensive demonstration video will be produced. This walkthrough presents the core features and user experience across multiple mission scenarios, illustrating how natural language dialogue seamlessly controls orbital simulations in augmented reality.

The demonstration includes:
\begin{itemize}
    \item \textbf{Interaction Flow:} A complete cycle from voice command input through agent interpretation to AR visualization and auditory feedback
    \item \textbf{Mission Showcase:} Examples from different mission contexts (ISS, GPS constellation, Hubble telescope, Voyager deep space) demonstrating the system's versatility
    \item \textbf{Educational Scenarios:} Representative tasks such as creating circular and elliptical orbits, adjusting simulation parameters, and exploring orbital mechanics concepts through conversational guidance
    \item \textbf{Technical Capabilities:} Display of the platform's physically accurate trajectory calculations, spatial anchoring, and real-time responsiveness
\end{itemize}

This demonstration serves both as documentation of the functional prototype and as a reference for potential users and contributors to understand the system's design and capabilities before engaging with the codebase.

\subsection{Open-Source Release}

The platform will be released as open-source software on GitHub, making the complete implementation—Unity project, C\# scripts, agent prompts, configuration templates, and documentation—publicly accessible. This release strategy serves multiple strategic objectives:

\begin{itemize}
    \item \textbf{Community Validation:} Educators, students, and developers can test the platform in their own contexts, providing authentic feedback on its educational effectiveness and technical robustness
    \item \textbf{Accessibility and Extensibility:} Users can adapt the system for different educational levels, add new mission scenarios, integrate alternative celestial bodies, or extend the agent's capabilities to suit specific pedagogical goals
    \item \textbf{Collaborative Development:} The open-source model invites contributions from the community, enabling iterative improvement and evolution beyond the scope of this initial work
    \item \textbf{Transparent Implementation:} Full access to source code allows technical review, replication of results, and learning from the implementation itself as an educational resource
\end{itemize}

\paragraph{User Configuration}

To maintain accessibility while respecting API service constraints, users provide their own API keys for OpenAI (language model) and ElevenLabs (text-to-speech) services. This approach ensures that the platform remains available to anyone interested in exploring or adapting it without imposing unsustainable service costs, while also giving users control over their model selection and usage limits.

\paragraph{Documentation and Support}

The repository includes comprehensive setup instructions, architectural documentation, and usage guidelines to lower the barrier for community adoption and contribution. This documentation covers Unity environment configuration, Meta Quest 3 deployment procedures, API integration, and the modular system architecture that facilitates extension.

This demonstration and release strategy transforms the platform from a single-instance prototype into a foundation for ongoing exploration and refinement by the broader educational technology and aerospace communities.