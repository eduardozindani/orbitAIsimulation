\section{Design Philosophy and Approach}
\label{sec:design_philosophy}

The development of this project is fundamentally an exploratory research endeavour into a new paradigm of human-computer interaction for educational purposes. Given the innovative and complex nature of integrating generative AI, augmented reality, and embodied interfaces, a rigid, waterfall-style development plan would be inappropriate. Instead, the methodology is guided by a philosophy that embraces iteration and modularity to navigate the technical challenges and discovery process inherent in such work.

The approach is defined by three core principles:
\begin{itemize}
    \item \textbf{Prototype-Driven:} The primary goal is the creation of a functional prototype that demonstrates the feasibility and potential of the proposed system. This approach prioritizes implementing the core functionalities of the user experience over exhaustive feature development, allowing for tangible and testable results that can validate the project's central thesis.
    \item \textbf{Iterative Development:} The project will be built in iterative cycles, following a process of building a core feature, testing its performance and usability, and refining it based on the results. This allows for flexibility in the implementation details, acknowledging that the optimal solutions for agent prompting and user interaction will be discovered and improved upon throughout the development lifecycle.
    \item \textbf{Modular Architecture:} The system is designed as a collection of distinct yet interconnected modules: the generative agent (the "brain") and the simulation and visualisation engine (the "world"). This modularity, a key objective of this project, makes the complex system manageable, facilitates parallel development and testing of components, and ensures the final architecture is extensible for future work.
\end{itemize}

\section{System Architecture and Data Flow}
\label{sec:system_architecture}

This section describes the system's architecture and how information flows between the user, the conversational agent, and the visualization engine to create an integrated educational experience.

\paragraph{Architectural Philosophy}

The system is designed around voice-first interaction because spoken dialogue is the most natural way humans seek and share knowledge. Rather than requiring users to learn complex interfaces or command syntax, the platform allows them to simply ask questions and request visualizations as they would when speaking with a knowledgeable instructor.

The architecture separates conversational intelligence from spatial visualization, enabling each component to excel at its specific purpose. The agent interprets intent, reasons about orbital mechanics, and guides learning through dialogue. The simulation engine translates these conversations into visual demonstrations, rendering orbital trajectories in three-dimensional space where users can observe and explore them.

\paragraph{Interaction Flow}

User interactions follow a continuous conversational loop. The user speaks a request—perhaps asking to see a specific orbit or questioning why certain parameters produce particular behaviors. The agent, powered by OpenAI's language models, interprets this natural language input and determines the appropriate response, whether that means creating a visualization, explaining a concept, or routing the user to a relevant learning resource.

When visualization is needed, the agent communicates with the Unity-based simulation engine to generate the requested orbital trajectory. The simulation calculates physically accurate paths using astrodynamics principles and renders them in the virtual reality environment. The user sees the orbit appear in space, anchored to their surroundings through the Meta Quest 3 headset.

The agent then provides spoken feedback via ElevenLabs voice synthesis, confirming what was created and offering educational context. This completes one cycle of the interaction loop, with the user free to ask follow-up questions, request modifications, or explore new concepts.

This architecture enables seamless integration between abstract physics concepts and tangible spatial representations, allowing users to learn through the natural combination of conversation and visual exploration.

\section{Core Component Implementation}
\label{sec:core_components}

The system architecture is realized through two distinct yet deeply integrated components. This separation enables each component to focus on its core responsibility while working together to create the complete educational experience.

\subsection{The Generative Agent (The "Brain")}
\label{subsec:agent}

\paragraph{Purpose and Responsibility}

The conversational agent serves as the educational intelligence of the platform. Its fundamental purpose is to bridge the gap between natural human communication and precise simulation commands, transforming vague or exploratory questions into specific orbital visualizations and educational explanations.

This component must interpret ambiguous user intent—understanding whether "show me how satellites work" should produce a specific orbit example, route to a mission specialist, or provide a conceptual explanation. It maintains conversational context across multiple turns, remembers what has been discussed, and adapts its guidance to match the user's demonstrated knowledge level.

The agent embodies different characters depending on context: Mission Control in the Hub, or mission specialists in the educational spaces. Each character has distinct personality and expertise, created through careful prompt design that defines their communication style, knowledge domain, and educational approach.

\paragraph{Technical Foundation}

The agent's reasoning capabilities are powered by OpenAI's language models, which provide natural language understanding and generation. Voice synthesis through ElevenLabs transforms text responses into spoken dialogue with character-specific voices, creating the auditory personality that makes each character feel distinct and engaging.

The conversational intelligence operates by interpreting user requests, determining appropriate actions (creating orbits, explaining concepts, routing to specialists), and generating educational responses that contextualize what the user sees. This reasoning process must balance precision—ensuring orbital parameters are physically valid—with flexibility to handle the natural variability of spoken language.

\subsection{The Simulation and AR Visualisation (The "World")}
\label{subsec:simulation}

\paragraph{Purpose and Responsibility}

The simulation component translates abstract orbital mechanics into tangible visual experiences. Its fundamental purpose is to make invisible physics visible—showing users what a 400 km orbit actually looks like in space, how inclination affects ground coverage, and how different mission requirements manifest as specific trajectory shapes.

This component must render physically accurate orbital paths while presenting them in ways that are visually comprehensible and educationally meaningful. The orbits must be mathematically correct according to astrodynamics principles, yet scaled and styled for effective learning rather than literal representation.

The visualization creates the spatial context where learning happens. By anchoring orbital trajectories to the user's physical environment through augmented reality, or placing users inside immersive virtual space, the component transforms orbital mechanics from equations on paper into three-dimensional phenomena users can walk around, observe from multiple angles, and genuinely inhabit.

\paragraph{Technical Foundation}

The simulation is built using Unity, a 3D development engine that provides the tools for calculating orbital trajectories and rendering them in virtual and augmented reality. The physics implementation, written in C\#, applies fundamental astrodynamics equations—primarily the two-body problem—to compute satellite positions and velocities over time.

Deployment targets the Meta Quest 3 headset, which enables both immersive VR experiences and AR visualizations through its passthrough camera system. This hardware choice provides standalone wireless operation and sufficient processing power for real-time orbital calculations and high-quality rendering, allowing users to engage with the simulation untethered and freely mobile.

\section{Development and Version Control}
\label{sec:version_control}

The project follows systematic development practices using GitHub for version control. All source code—including Unity C\# scripts, prompt templates, and configuration files—is tracked in a central repository, providing complete history of changes and enabling experimental work through branching without compromising the main project stability. This systematic approach aligns with the iterative development philosophy, where each development cycle's progress is documented and preserved.

\section{Evaluation Plan}
\label{sec:evaluation_plan}

\paragraph{Dual Assessment Rationale}

The platform's success must be measured along two dimensions: technical functionality and educational effectiveness. Technical validation alone cannot confirm whether the system actually helps users learn, while educational assessment without technical verification cannot determine if observed outcomes result from reliable, consistent system behavior or from chance. This dual approach ensures the platform is both technically sound and pedagogically valuable.

\subsection{Technical Validation}

Technical evaluation verifies that the system performs reliably and correctly. This validation establishes confidence that observed learning outcomes stem from consistent, accurate system behavior rather than from unpredictable or incorrect responses.

Key validation areas include:
\begin{itemize}
    \item \textbf{Conversational Accuracy:} The agent correctly interprets user intent and executes appropriate simulation functions across diverse natural language inputs
    \item \textbf{Physics Fidelity:} Rendered orbital trajectories match validated astrodynamics calculations
    \item \textbf{Interaction Responsiveness:} Voice input to visual and auditory feedback occurs quickly enough to maintain conversational flow
    \item \textbf{Spatial Stability:} Augmented reality visualizations remain properly anchored in the user's physical environment
\end{itemize}

These technical metrics ensure the platform provides a stable foundation for learning rather than introducing confusion through inconsistent or incorrect behavior.

\subsection{Educational Effectiveness Assessment}

Educational evaluation investigates whether the platform achieves its fundamental purpose: helping users develop genuine understanding of orbital mechanics principles.

This assessment employs a small-scale qualitative study with 3-5 undergraduate students from aerospace engineering or related fields. Participants engage with the platform through exploratory tasks—creating different orbit types, visiting mission showcases, asking questions about observed phenomena—then provide feedback through semi-structured interviews and questionnaires.

The evaluation focuses on whether the platform makes orbital concepts more intuitive and comprehensible compared to traditional learning methods like textbooks and diagrams. Questions probe engagement, ease of use, and most critically, whether users feel they developed better mental models of how orbits work through the interactive, spatial experience.

This qualitative approach prioritizes depth of insight over statistical significance. Rich feedback from a few users who thoughtfully engage with the system provides more actionable understanding of educational effectiveness than surface-level metrics from larger samples. The goal is to validate the core educational premise and identify opportunities for refinement.