\section{Design Philosophy and Approach}
\label{sec:design_philosophy}

The development of this project is fundamentally an exploratory research endeavour into a new paradigm of human-computer interaction for educational purposes. Given the innovative and complex nature of integrating generative AI, immersive mixed reality, and embodied interfaces, a rigid, waterfall-style development plan would be inappropriate. Instead, the methodology is guided by a philosophy that embraces iteration and modularity to navigate the technical challenges and discovery process inherent in such work.

The approach is defined by three core principles:
\begin{itemize}
    \item \textbf{Prototype-Driven:} The primary goal is the creation of a functional prototype that demonstrates the feasibility and potential of the proposed system. This approach prioritizes implementing the core functionalities of the user experience over exhaustive feature development, allowing for tangible and testable results that can validate the project's central thesis.
    \item \textbf{Iterative Development:} The project will be built in iterative cycles, following a process of building a core feature, testing its performance and usability, and refining it based on the results. This allows for flexibility in the implementation details, acknowledging that the optimal solutions for agent prompting and user interaction will be discovered and improved upon throughout the development lifecycle.
    \item \textbf{Modular Architecture:} The system is designed as a collection of distinct yet interconnected modules: the generative agent (the "brain") and the simulation and visualisation engine (the "world"). This modularity, a key objective of this project, makes the complex system manageable, facilitates parallel development and testing of components, and ensures the final architecture is extensible for future work.
\end{itemize}

These principles guided a four-phase development strategy. Phase 1 established the conversational agent in the Hub environment with circular and elliptical orbit creation capabilities. Phase 2 implemented three Mission Spaces (ISS, Hubble, Voyager) with specialist agents and scene transition tools. Phase 3 integrated bidirectional voice through ElevenLabs (Scribe API for speech-to-text, TTS for character synthesis). Phase 4 deployed the complete system to Meta Quest 3 with immersive VR visualization. Each phase produced a testable, working system that integrated seamlessly with subsequent development without requiring architectural changes.

\section{System Architecture and Data Flow}
\label{sec:system_architecture}

The platform architecture comprises two integrated spaces: the "Hub" (Mission Control) where users create custom orbits through conversation, and "Mission Spaces" (ISS, Hubble, Voyager) where specialists demonstrate real missions. Users navigate between these environments via voice commands (\texttt{route\_to\_mission}, \texttt{return\_to\_hub}), with conversational context preserved across transitions.

The system separates conversational intelligence (OpenAI GPT-4.1 via Responses API) from spatial visualization (Unity 3D), connected through a tool-calling interface. The agent interprets natural language, selects appropriate tools (\texttt{create\_circular\_orbit}, \texttt{create\_elliptical\_orbit}, \texttt{set\_simulation\_speed}, \texttt{pause\_simulation}, \texttt{reset\_simulation\_time}, \texttt{clear\_orbit}, \texttt{route\_to\_mission}, \texttt{return\_to\_hub}), and Unity executes the corresponding physics calculations and rendering.

\subsection{Interaction Flow}
\label{subsec:interaction_flow}

User interactions follow one of two primary workflow patterns depending on intent: orbit manipulation or scene navigation. Both share common stages (speech input, AI reasoning, audio output) but diverge in tool execution and timing.

\subsubsection{Orbit Creation Workflow}

When users create or modify orbits (\texttt{create\_circular\_orbit}, \texttt{create\_elliptical\_orbit}, \texttt{set\_simulation\_speed}), the interaction follows eight steps:

\begin{enumerate}
    \item User speaks orbital request via Quest 3 microphone
    \item ElevenLabs Scribe API transcribes audio to text
    \item OpenAI GPT-4.1 interprets intent and selects appropriate orbit tool
    \item Unity's \texttt{ToolExecutor} validates parameters and invokes \texttt{OrbitController}
    \item \texttt{OrbitController} calculates trajectory using vis-viva equation and renders visualization in current scene
    \item Tool execution result (orbital parameters, velocity, period) returns to LLM
    \item Agent generates educational response explaining the created orbit
    \item ElevenLabs TTS synthesizes voice and audio plays through Quest 3
\end{enumerate}

\subsubsection{Navigation Workflow}

When users navigate between environments (\texttt{route\_to\_mission}, \texttt{return\_to\_hub}), the interaction extends to nine steps:

\begin{enumerate}
    \item User speaks navigation request via Quest 3 microphone
    \item ElevenLabs Scribe API transcribes audio to text
    \item OpenAI GPT-4.1 interprets intent and selects navigation tool
    \item Unity's \texttt{ToolExecutor} validates destination and prepares transition
    \item Tool execution result confirms target scene
    \item Agent generates contextual transition response (e.g., ``Connecting you to ISS specialist'')
    \item ElevenLabs TTS synthesizes character-specific voice (Mission Control's authority, specialist's expertise)
    \item Audio plays through Quest 3 spatial audio system
    \item \textbf{After audio completes}, \texttt{SceneTransitionManager} loads Mission Space with conversation context preserved
\end{enumerate}

Both workflows maintain conversation history across turns via \texttt{MissionContext} singleton, enabling contextual dialogue. The critical distinction: orbit tools execute within the current scene (immediate visual feedback), while navigation tools trigger asynchronous scene transitions after AI response completes (preventing audio cutoff mid-sentence). Users control all aspects—orbit creation, mission navigation, simulation parameters—through voice alone.

\section{Mixed Reality Design Rationale}
\label{sec:mr_rationale}

While the Quest 3 hardware supports both VR and AR passthrough capabilities, the platform implementation focuses exclusively on immersive VR for pedagogical reasons. Orbital mechanics involves scales (420 km for ISS, 35,786 km for geostationary orbit) incompatible with domestic spaces—AR overlays would show trajectories passing through walls and furniture, creating perceptual friction between room-scale and cosmic-scale contexts. VR isolation places users within the orbital environment itself, establishing coherent spatial context where Earth floats in space and trajectories exist in their natural domain. The architecture could potentially support future AR passthrough implementation for collaborative learning, museum installations, or classroom demonstrations where physical anchoring adds value, though this extension was not implemented in the current work.

\section{Technical Implementation}
\label{sec:core_components}

The system implements conversational AI (OpenAI GPT-4.1 for reasoning, ElevenLabs for voice synthesis/transcription) interfaced with Unity 6 (6000.0.47f1) physics simulation. The agent embodies four characters—Mission Control in the Hub, plus three mission specialists (ISS crew perspective for LEO operations, Hubble engineer for telescope mission design, Voyager/Sagan persona for interplanetary trajectories)—each with distinct voice profiles and expertise areas. Unity implements two-body orbital physics in C\#, calculates trajectories via vis-viva equation, and renders visualizations on Quest 3. The platform supports circular orbit creation (160-35,786 km altitude) and elliptical orbit creation (periapsis/apoapsis 160-100,000 km), with inclination constrained to 0-180°.

\section{Development and Version Control}
\label{sec:version_control}

The project follows systematic development practices using GitHub for version control. All source code—including Unity C\# scripts, prompt templates, and configuration files—is tracked in a central repository, providing complete history of changes and enabling experimental work through branching without compromising the main project stability. This systematic approach aligns with the iterative development philosophy, where each development cycle's progress is documented and preserved.

\section{Validation Strategy}
\label{sec:evaluation_plan}

The platform's technical feasibility and system integration are validated through demonstration of complete interaction scenarios across all mission spaces, showing that the proposed architecture functions reliably in representative educational workflows. The validation approach prioritizes operational evidence: users exploring orbital mechanics through natural conversation, navigating between Hub and Mission Spaces, and experiencing the full voice-driven VR cycle. Implementation details are documented and made publicly available to enable independent verification of feasibility claims. Users provide their own API keys for OpenAI and ElevenLabs services. Comprehensive documentation covers Unity configuration, Quest 3 deployment, and integration procedures.