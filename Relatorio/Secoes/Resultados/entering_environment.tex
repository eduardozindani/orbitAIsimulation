The platform's learning journey begins when the user puts on the Quest 3 headset and enters the virtual environment. This onboarding sequence—cutscene introduction, Hub arrival, and first interaction—establishes spatial context, introduces the conversational agent, and demonstrates the voice-driven interface that will mediate all subsequent exploration.

\subsection{Opening Sequence: Narrator Introduction}
\label{subsec:opening_sequence}

The experience opens with a cinematic sequence. The user's viewpoint begins in deep space, far from Earth, while a narrator's voice provides context:

\begin{quote}
\textit{NARRATOR:} Above us, thousands of satellites trace perfect arcs—testaments to gravity, velocity, and the geometry of motion. You're about to join that tradition. I'll guide the physics; you'll design the orbits. Together, we'll explore how spacecraft navigate the cosmos.
\end{quote}

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{Imagens/cut_scene.png}
\caption{Opening cutscene frame showing Earth from deep space. The camera begins at this distant perspective before zooming toward the Hub's orbital position.}
\label{fig:cut_scene}
\end{figure}

As the narrator speaks, the camera moves smoothly toward Earth over 28.7 seconds, establishing the cosmic scale while framing the interaction paradigm. The phrase ``I'll guide the physics; you'll design the orbits'' positions the learner as active participant rather than passive observer.

\subsection{Hub Environment Arrival: Spatial Presence and Agent Welcome}
\label{subsec:hub_arrival}

When the cutscene concludes, the user arrives at the Hub—Mission Control's orbital workspace. The transition from cinematic observer to embodied presence is immediate: the user now floats in space, surrounded by stars, with Earth directly ahead.

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{Imagens/intro.png}
\caption{Hub environment from user perspective. Earth floats in stereoscopic three-dimensional space, rendered with 8K textures to provide visual fidelity while maintaining stable frame rate on Quest 3.}
\label{fig:intro}
\end{figure}

\FloatBarrier

CAPCOM greets the user, establishing orientation and interaction mechanics. The welcome message orients the user spatially (``floating above Earth''), introduces push-to-talk voice input (A button), explains available capabilities (orbit design and specialist consultation), and previews the three Mission Spaces—ISS, Hubble, and Voyager (full greeting in Appendix~\ref{appendix:transcript}). CAPCOM's professional warmth distinguishes Mission Control from the three specialists who will appear with their own vocal identities and expertise.

\subsection{Satellite Discovery: First Interaction}
\label{subsec:satellite_discovery}

After orienting to the environment, the user notices a spacecraft model floating near Earth's surface. This three-dimensional satellite will serve as the visual anchor for all orbital trajectories created during the session. The user moves closer to examine it, then presses the A button to speak:

\begin{quote}
\textit{USER:} Hey, Capcom. I see we have a satellite here. Is this the satellite that we can build?
\end{quote}

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{Imagens/satelite_zoomed_in.png}
\caption{Close-up view of the simulation satellite in the Hub environment. The user has navigated their perspective to examine the spacecraft model.}
\label{fig:satellite_zoomed}
\end{figure}

\FloatBarrier

This first voice interaction exercises the complete bidirectional speech pipeline. The agent responds in conversational mode:

\begin{quote}
\textit{CAPCOM:} Hello from Mission Control. Yes—this is your simulation satellite. We can build orbits by setting circular or elliptical paths. Tell me either the altitude for circular, or periapsis and apoapsis for elliptical, and I'll get it launched.
\end{quote}

CAPCOM's response introduces orbital terminology: ``altitude'' defines circular orbits, while ``periapsis and apoapsis'' specify elliptical paths. The complete interaction cycle—button press, speech transcription, agent response, audio synthesis—completes in 4--6 seconds, establishing the conversational rhythm that will support exploratory learning throughout subsequent sections.
