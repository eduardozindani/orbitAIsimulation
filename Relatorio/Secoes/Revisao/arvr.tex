\section{Augmented and Virtual Reality in Immersive Educational Simulation Systems}
\label{sec:ar_vr_review}

Augmented Reality (AR) and Virtual Reality (VR) are complementary immersive technologies that enrich or replace a user’s perception of the world. AR overlays digital content onto the real environment in real-time, allowing virtual objects to coexist with physical surroundings \cite{billinghurst2015survey}. In contrast, VR completely immerses the user in a fully synthetic, computer-generated environment, blocking out the physical world. Milgram’s classic “Reality-Virtuality” continuum illustrates these as end-points: AR lies near the real-world end (mixing virtual content with reality), whereas VR occupies the extreme virtual end with an entirely simulated world \cite{milgram1994}. In essence, AR adds to the user’s real-world experience, while VR transposes the user into an interactive virtual scene. Both technologies share common roots in decades of research and development. The term augmented reality was first coined by Caudell and Mizell (1992) in the context of assisting Boeing manufacturing with see-through displays \cite{caudell1992}. A few years later, Azuma’s influential survey defined AR by three key characteristics: combining real and virtual content, interactive operation in real time, and accurate 3D registration of virtual objects in the physical world \cite{azuma1997, billinghurst2015survey}. VR, meanwhile, has been long conceptualized as achieving presence – the feeling of “being there” in a virtual environment – by engaging multiple senses with responsive 3D graphics and audio \cite{johnson2018}. Modern definitions emphasize that VR provides immersive first-person experiences where users can interact with simulated worlds as if they were real, inducing a strong sense of presence and agency within the virtual scene.

\subsection{Hardware Evolution:}
AR and VR technologies have evolved rapidly, enabling consumer-grade devices that support realistic immersive experiences. While early head-mounted displays date back to the 1960s (e.g., Sutherland’s Sword of Damocles), the 2010s marked a turning point with modern devices. On the VR front, the Oculus Rift prototype (2010) by Palmer Luckey re-ignited interest with a wide field of view and affordable design. Crowdfunded in 2012 and acquired by Facebook in 2014, Oculus released its first consumer headset in 2016, alongside HTC’s Vive, which introduced room-scale tracking. These devices brought high-fidelity visuals and motion tracking to mainstream audiences.

The next major step came with standalone VR headsets. The Oculus/Meta Quest series, starting in 2019, integrated processing and inside-out tracking directly into the headset. Quest 2 (2020) and Quest 3 (2023) improved resolution, optics, and added passthrough AR capabilities \cite{ruth2024}. In parallel, PC-based headsets like the Valve Index and Varjo pushed the fidelity frontier for gaming and enterprise simulation.

AR hardware followed a distinct trajectory. Initial systems used handheld or laptop setups, but the release of Microsoft’s HoloLens in 2016 marked the arrival of self-contained AR headsets with spatial mapping and inside-out tracking. Magic Leap One (2018) added novel display technologies \cite{billinghurst2015survey}, while consumer experiments like Google Glass (2013) explored heads-up interfaces before being discontinued in 2023 \cite{ruth2024}. 

Smartphones played a critical role in scaling AR adoption. Apps like Pokémon GO (2016) introduced mainstream users to AR through camera overlays. ARKit (Apple) and ARCore (Google), launched in 2017, enabled mobile AR with motion and depth tracking \cite{vieyra2018}.

Most recently, the line between AR and VR is blurring. Apple’s Vision Pro (announced 2023) merges high-resolution VR with passthrough AR, positioning itself as a “spatial computer.” With features like dual 4K displays and hand/eye tracking, it may represent a watershed moment for XR despite its premium price \cite{ruth2024}.

As of 2025, the hardware ecosystem spans from mobile-based AR apps to advanced mixed reality headsets, forming a robust toolbox for immersive educational simulations.

\subsection{Software Ecosystems and Frameworks:} Alongside hardware, a mature software ecosystem has enabled rapid development of immersive simulations. Modern game engines such as Unity and Unreal Engine have become the de facto platforms for AR/VR content creation. These engines provide high-performance 3D graphics rendering, physics simulation, and cross-platform deployment, greatly simplifying the creation of interactive virtual environments. Unity, for example, offers an entire XR development toolkit (with support for VR headsets and AR through packages like AR Foundation) that abstracts away device-specific details and allows developers to build an application once and deploy across multiple headsets \cite{atta2022}. Unreal Engine likewise includes integrated support for VR rendering and AR (via ARKit/ARCore plugins), making high-fidelity visualization accessible to developers in academia and industry.

For mobile AR, platform-specific frameworks are key. Apple’s ARKit (introduced in iOS 11, 2017) and Google’s ARCore (for Android, 2017) brought advanced AR capabilities to hundreds of millions of smartphones \cite{vieyra2018}. These software development kits handle real-time tracking of the device’s position, surface detection, lighting estimation, and more, allowing apps to place and persist virtual objects in the user’s environment. Thanks to ARKit/ARCore, an educator can deploy an AR simulation on standard tablets or phones – for instance, letting students point an iPad at a textbook and see 3D molecules or physical field lines appear “attached” to the pages. On the web, the WebXR API has emerged as a W3C standard enabling AR and VR experiences to run directly in web browsers using JavaScript \cite{webxr2021}. WebXR (successor to earlier WebVR/WebAR efforts) allows an immersive educational module to be accessed with a simple URL, lowering the barrier to entry (no app install required) and ensuring compatibility across different devices (from VR headsets to phones). This is particularly relevant for broad educational deployments, where web-based delivery can be more practical. Complementing these are various supporting frameworks: for example, libraries for spatial mapping, hand tracking, and user interaction (e.g. Microsoft’s Mixed Reality Toolkit for Unity, or Vuforia for image-target AR) which provide higher-level tools for common AR/VR interactions. There are also open standards like OpenXR (released by the Khronos Group in 2019) that unify the interface to VR/AR hardware – a developer can write code once against OpenXR and run on any compliant headset (Oculus, SteamVR, Windows Mixed Reality, etc.), which is increasingly adopted by engines and platforms. In summary, the software landscape – from powerful 3D engines to AR phone toolkits and web standards – has matured to a point that immersive educational simulations can be built with relatively modest effort compared to a decade ago. This thesis will leverage these tools to construct its simulation system, ensuring it is built on proven, widely supported technology.

\subsection{Use Cases in Education:} 
AR and VR have shown strong potential to enhance learning, particularly in subjects involving abstract or spatial concepts. Their core strength lies in making the invisible visible and the abstract tangible. In physics education, for instance, VR has helped students visualize and manipulate 3D vectors, improving understanding of vector addition and spatial relationships \cite{campos2022}. Studies show that such immersive tools can boost engagement and deepen comprehension of abstract STEM topics like electromagnetism or geometry through interactive, risk-free exploration \cite{campos2022, johnson2018}.

In astronomy and aerospace, where scales are far beyond human experience, immersive technologies offer unique advantages. VR enables virtual field trips through space — letting students stand on Mars or orbit planets — providing an intuitive grasp of scale and distance. Learners can explore the solar system with accurate proportions, making complex spatial relationships (like planetary distances or ring sizes) more comprehensible \cite{atta2022}. Astrophysical phenomena such as orbital mechanics and black hole dynamics are also made more accessible through interactive VR visualizations.

In aerospace engineering, VR and AR are increasingly used for hands-on training. Beyond traditional flight simulators, modern VR platforms allow students to perform simulated pre-flight inspections, engine maintenance, or spacecraft docking. Vaughn College, for example, uses VR for aviation trainees to practice inspecting and assembling parts, reinforcing mechanical familiarity before real-world exposure. Similarly, Atta et al. (2022) created a virtual “space lab” where students assemble a CubeSat in a simulated cleanroom, boosting their understanding of subsystem configuration through direct interaction and gamified tasks \cite{atta2022}.

AR complements this by overlaying digital instructions on real-world hardware. NASA’s Project Sidekick exemplifies this: astronauts use HoloLens headsets aboard the ISS to receive real-time, spatially anchored maintenance guidance \cite{NASA2015}. In classrooms, AR enables students to interact with 3D models of rockets or overlay CAD designs onto physical parts, enriching theoretical lessons with live, contextual visualization \cite{milgram1994, atta2022}.


\subsection{Embodiment, Interaction, and Spatial Cognition:} A recurring theme in the educational use of AR/VR is the role of embodied and spatial learning. Immersive technologies engage the human sensorimotor system – users move their bodies to navigate virtual spaces, use gestures to interact with virtual objects, and perceive environments at true scale. This physicality supports cognitive processing by leveraging innate spatial reasoning and muscle memory. The theory of embodied cognition holds that learning is grounded in the body’s interactions with its environment, and AR/VR extend this principle digitally. Johnson-Glenberg (2018) highlights the pedagogical value of 3D gestures: when learners rotate a virtual object or walk through a graph, they build stronger memory links \cite{johnson2018}. Her research shows that full-motion VR, where body movements align with abstract concepts, can deepen understanding and recall. Complementary studies (e.g., Liu et al., 2020) found improved retention when students enacted phenomena physically, and also noted increased presence and agency—factors tied to motivation \cite{johnson2018, campos2022}.

Spatial cognition benefits are also well-documented. VR’s stereoscopic depth and six degrees of freedom help learners perceive complex spatial relationships, vital in subjects like anatomy, geography, and engineering. Students exploring a molecule or a solar system in VR can shift perspective freely, activating spatial memory and supporting what researchers call “situated learning” – knowledge acquired in rich spatial contexts becomes more intuitive and transferable. Campos et al. (2022), for example, found that immersive 3D interaction notably enhanced vector learning tasks requiring spatial reasoning \cite{campos2022}. Similarly, in astronomy, VR’s ability to scale from the Milky Way to Earth provides concrete visualizations of abstract systems (Kersting et al., 2024).

While AR/VR offer compelling tools, they are not magic bullets – user comfort, software complexity, and thoughtful pedagogical integration remain critical \cite{johnson2018}. Still, evidence shows that immersive simulations can enhance traditional teaching, especially for learning goals involving visualization, experimentation, or embodied experience. In the context of this thesis, the implications are clear: AR and VR form a foundational layer. They enable students to interact with simulations of aerospace systems—such as satellites or orbital dynamics—in an intuitive and experiential manner. As hardware becomes lighter and more capable, and software ecosystems more robust, immersive tools are becoming increasingly viable in education. With spatial computing platforms entering mainstream use \cite{ruth2024}, AR and VR are poised not just as delivery platforms but as new paradigms for engaging with knowledge.

\paragraph{Application to This Work}

This thesis leverages the pedagogical strengths of immersive spatial learning through Virtual Reality as the primary modality. While the platform architecture is built on mixed reality hardware (Meta Quest 3) that supports both VR and AR passthrough modes, the educational experience emphasizes full VR immersion for the reasons detailed in Section \ref{sec:mr_rationale}. The literature reviewed here establishes the broader context of immersive educational technologies, while the implementation focuses specifically on VR's capacity to place learners inside coherent spatial environments optimized for understanding orbital mechanics.